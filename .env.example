# Yap Text Inference — Environment Variables
#
# Copy to .env and fill in the values you need.
# Every variable has a sensible default — only set what you want to override.
# All variables are commented out so this file is safe to copy as-is.

# =============================================================================
# Deployment & Models
# =============================================================================

# API key clients must supply to authenticate WebSocket connections.
TEXT_API_KEY=

# Deployment mode: "both" (chat + tool), "chat", or "tool".
# DEPLOY_MODE=both

# HuggingFace model ID for the chat engine (required when mode includes chat).
CHAT_MODEL=

# HuggingFace model ID for the tool classifier (required when mode includes tool).
TOOL_MODEL=

# Inference backend: "trt" (TensorRT-LLM) or "vllm".
# INFERENCE_ENGINE=trt

# =============================================================================
# GPU & Memory
# =============================================================================

# Fraction of GPU memory allocated to the chat engine.
# Default: 0.70 in "both" mode, 0.90 in single-engine mode.
# CHAT_GPU_FRAC=0.70

# Fraction of GPU memory allocated to the tool classifier.
# Default: 0.20 in "both" mode, 0.90 in single-engine mode.
# TOOL_GPU_FRAC=0.20

# KV-cache data type: "auto", "fp8", or explicit dtype string.
# CHAT_KV_DTYPE=auto

# Override GPU SM architecture (e.g., "89" for Ada). Auto-detected if empty.
# GPU_SM_ARCH=

# =============================================================================
# TensorRT-LLM
# =============================================================================

# Path to pre-built TRT engine directory (containing rank0.engine, etc.).
# TRT_ENGINE_DIR=

# Path to TRT checkpoint directory (used during engine build).
# TRT_CHECKPOINT_DIR=

# Path to the TRT-LLM repo (for build scripts).
# TRT_REPO_DIR=

# Maximum batch size for TRT engine build. Auto-derived if unset.
# TRT_MAX_BATCH_SIZE=

# Runtime batch size. Auto-derived if unset.
# TRT_BATCH_SIZE=

# Maximum input sequence length (default: CHAT_MAX_LEN = 5025).
# TRT_MAX_INPUT_LEN=5025

# Maximum output sequence length (default: CHAT_MAX_OUT = 150).
# TRT_MAX_OUTPUT_LEN=150

# TRT engine dtype: "float16", "bfloat16", etc.
# TRT_DTYPE=float16

# Fraction of GPU memory for KV cache (default: CHAT_GPU_FRAC).
# TRT_KV_FREE_GPU_FRAC=

# AWQ quantization block size.
# TRT_AWQ_BLOCK_SIZE=128

# Number of calibration samples for quantization.
# TRT_CALIB_SIZE=64

# Calibration sequence length (default: CHAT_MAX_LEN + CHAT_MAX_OUT).
# TRT_CALIB_SEQLEN=5175

# Calibration batch size.
# TRT_CALIB_BATCH_SIZE=16

# =============================================================================
# vLLM
# =============================================================================

# Enable vLLM V1 engine (1 = on, 0 = off).
# VLLM_USE_V1=1

# Attention backend: "FLASHINFER", "XFORMERS", or auto-detected.
# VLLM_ATTENTION_BACKEND=

# Disable CUDA graphs for debugging (1 = eager mode).
# ENFORCE_EAGER=0

# Maximum batched tokens for the chat engine. "auto" or integer.
# MAX_NUM_BATCHED_TOKENS_CHAT=256

# =============================================================================
# Chat Behavior
# =============================================================================

# Prefix prepended when the tool classifier detects a screen-check request.
# CHECK_SCREEN_PREFIX=MUST CHECK SCREEN:

# Prefix prepended when the screen has already been checked.
# SCREEN_CHECKED_PREFIX=ON THE SCREEN NOW:

# Enable <thinking> tags in chat template (for reasoning models).
# CHAT_TEMPLATE_ENABLE_THINKING=false

# Interval between periodic vLLM cache resets (seconds).
# CACHE_RESET_INTERVAL_SECONDS=600

# Minimum session duration before a disconnect triggers a cache reset (seconds).
# CACHE_RESET_MIN_SESSION_SECONDS=300

# =============================================================================
# Sampling Defaults
# =============================================================================

# Default sampling temperature.
# CHAT_TEMPERATURE=0.8

# Nucleus sampling top-p.
# CHAT_TOP_P=0.95

# Top-k sampling.
# CHAT_TOP_K=30

# Min-p sampling threshold.
# CHAT_MIN_P=0.0

# Repetition penalty (1.0 = no penalty).
# CHAT_REPETITION_PENALTY=1.0

# Presence penalty.
# CHAT_PRESENCE_PENALTY=0.0

# Frequency penalty.
# CHAT_FREQUENCY_PENALTY=0.0

# Path to a JSON file with per-token logit biases.
# CHAT_LOGIT_BIAS_FILE=

# =============================================================================
# Context & Token Limits
# =============================================================================

# Maximum input context length (tokens).
# CHAT_MAX_LEN=5025

# Maximum generation output length (tokens).
# CHAT_MAX_OUT=150

# Maximum tokens allowed in the chat system prompt.
# CHAT_PROMPT_MAX_TOKENS=1500

# Maximum tokens for conversation history.
# HISTORY_MAX_TOKENS=3000

# Maximum tokens for a single user utterance.
# USER_UTT_MAX_TOKENS=500

# =============================================================================
# Tool Classifier
# =============================================================================

# Enable language-based filtering before tool classification.
# TOOL_LANGUAGE_FILTER=true

# Confidence threshold for tool classification decisions.
# TOOL_DECISION_THRESHOLD=0.66

# Enable torch.compile for the tool model (may improve throughput).
# TOOL_COMPILE=false

# Maximum history tokens fed to the tool classifier.
# TOOL_HISTORY_TOKENS=1536

# Maximum input length for the tool classifier.
# TOOL_MAX_LENGTH=1536

# =============================================================================
# WebSocket
# =============================================================================

# Idle timeout before the server closes an inactive connection (seconds).
# WS_IDLE_TIMEOUT_S=150

# Interval for the idle-watchdog heartbeat check (seconds).
# WS_WATCHDOG_TICK_S=5

# Maximum concurrent WebSocket connections. Unset = unlimited.
# MAX_CONCURRENT_CONNECTIONS=

# Micro-buffer flush interval for streaming chunks (ms, 0 = no buffering).
# STREAM_FLUSH_MS=0

# Maximum messages per rate-limit window.
# WS_MAX_MESSAGES_PER_WINDOW=25

# Rate-limit window duration for messages (seconds).
# WS_MESSAGE_WINDOW_SECONDS=60

# Maximum cancel requests per rate-limit window.
# WS_MAX_CANCELS_PER_WINDOW=25

# Rate-limit window duration for cancels (seconds).
# WS_CANCEL_WINDOW_SECONDS=60

# =============================================================================
# Timeouts
# =============================================================================

# Maximum time for a single chat generation before timeout (seconds).
# GEN_TIMEOUT_S=60

# Maximum time to wait for a tool classification result (seconds).
# TOOL_TIMEOUT_S=10

# Idle TTL before sessions are garbage-collected (seconds).
# SESSION_IDLE_TTL_SECONDS=1800

# =============================================================================
# Logging
# =============================================================================

# Application log level: DEBUG, INFO, WARNING, ERROR, CRITICAL.
# APP_LOG_LEVEL=INFO

# =============================================================================
# HuggingFace
# =============================================================================

# HuggingFace API token for downloading private/gated models.
HF_TOKEN=

# =============================================================================
# Telemetry — Sentry (error reporting)
# =============================================================================
# Set SENTRY_DSN to enable. Leave empty or omit to run without Sentry.

# SENTRY_DSN=https://examplePublicKey@o0.ingest.sentry.io/0
# SENTRY_ENVIRONMENT=production
# SENTRY_RELEASE=
# SENTRY_SAMPLE_RATE=1.0

# =============================================================================
# Telemetry — Axiom (metrics + traces via OpenTelemetry)
# =============================================================================
# Set AXIOM_API_TOKEN to enable. Leave empty or omit to run without Axiom.

# AXIOM_API_TOKEN=xaat-xxxxxxxx
# AXIOM_DATASET=text-inference-api
# AXIOM_ENVIRONMENT=production

# =============================================================================
# Telemetry — Cloud platform & OTel tuning
# =============================================================================

# Cloud platform tag for fleet identification.
# CLOUD_PLATFORM=

# OpenTelemetry service name attached to every span and metric.
# OTEL_SERVICE_NAME=yap-text-inference-api

# Span batch flush interval (ms).
# OTEL_TRACES_EXPORT_INTERVAL_MS=5000

# Metric export interval (ms).
# OTEL_METRICS_EXPORT_INTERVAL_MS=15000

# Maximum spans per export batch.
# OTEL_TRACES_BATCH_SIZE=512
