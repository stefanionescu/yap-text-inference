version: '3.8'

services:
  yap-server:
    image: ${DOCKER_HUB_USERNAME:-your-username}/yap-text-inference:${TAG:-latest}
    container_name: yap-text-inference
    restart: unless-stopped
    
    # GPU support (requires nvidia-docker)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    
    # Environment variables for AWQ deployment
    # Only specify the variables you want to override - all have sensible defaults
    environment:
      # Required: Pre-quantized AWQ models from Hugging Face
      # Uncomment and specify your models:
      # - AWQ_CHAT_MODEL=your-org/chat-awq
      # - AWQ_TOOL_MODEL=your-org/tool-awq
      
      # Optional overrides (all have defaults):
      # - DEPLOY_MODELS=both          # both|chat|tool
      # - CONCURRENT_MODEL_CALL=1     # 0=sequential, 1=concurrent
      # - YAP_API_KEY=yap_token       # API authentication
      # - CHAT_GPU_FRAC=0.70          # GPU memory for chat model
      # - TOOL_GPU_FRAC=0.20          # GPU memory for tool model
    
    # Expose port
    ports:
      - "8000:8000"
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Resource limits
    shm_size: 2g
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864
    
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # Optional: monitoring container
  watchtower:
    image: containrrr/watchtower
    container_name: yap-watchtower
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_POLL_INTERVAL=300
      - WATCHTOWER_INCLUDE_RESTARTING=true
    profiles: ["monitoring"]

volumes:
  hf_cache:
    driver: local
  vllm_cache:
    driver: local
