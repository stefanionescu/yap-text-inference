# VLLM requirements

--extra-index-url https://download.pytorch.org/whl/cu130

# VLLM 0.13.0 with CUDA 13.0 support (PyPI wheel is CUDA 12, need cu130 wheel from GitHub; manylinux_2_35)
vllm @ https://github.com/vllm-project/vllm/releases/download/v0.13.0/vllm-0.13.0+cu130-cp38-abi3-manylinux_2_35_x86_64.whl
torch==2.9.0+cu130 ; sys_platform == 'linux'
llmcompressor==0.9.0
fla-core==0.4.0
accelerate==1.1.1
datasets==3.1.0
safetensors==0.4.5
sentencepiece==0.2.0
protobuf==5.28.2
pyyaml==6.0.2
rich==13.9.4
loguru==0.7.3
tqdm==4.67.1

fastapi==0.124.4
uvicorn[standard]==0.30.6
websockets==12.0
pydantic==2.12.0
httpx==0.27.2
uvloop==0.19.0
transformers==4.56.0
tokenizers==0.22.0
mistral_common==1.8.5
hf_transfer==0.1.8
huggingface_hub==0.34.0
ftfy==6.2.3
lingua-language-detector==2.1.0
phonenumbers==9.0.3
