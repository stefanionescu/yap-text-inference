# TensorRT-LLM requirements
# NOTE: torch and tensorrt_llm are installed via scripts/engines/trt/install.sh
# to ensure wheels match the installed CUDA toolkit (default: PyTorch 2.9.1+cu130)
# TensorRT-LLM 1.2.0rc4 requires CUDA 13.0 and torch 2.9.x

# Core server dependencies (same as vLLM)
fastapi==0.115.4
uvicorn[standard]==0.30.6
websockets==12.0
orjson==3.10.7
pydantic==2.12.0
httpx==0.27.2
uvloop==0.19.0

# HuggingFace and ML (versions pinned to match TRT-LLM 1.2.0rc4 requirements)
# NOTE: tokenizers is NOT pinned - let transformers pull the compatible version
# transformers 4.53.1 requires tokenizers>=0.21,<0.22
huggingface_hub==0.36.0
hf_transfer==0.1.8
transformers==4.53.1
accelerate==1.1.1
safetensors==0.4.5
datasets==3.1.0

# Tokenizer and text processing
sentencepiece==0.2.0
protobuf==5.28.2
mistral_common==1.8.5
ftfy==6.2.3
lingua-language-detector==2.1.0

# TensorRT-LLM dependencies
mpi4py==3.1.6
cuda-python==13.0.0
numpy==1.26.4

# Build tools
ninja==1.13.0
packaging==25.0
setuptools==79.0.1
wheel==0.45.1

# Utilities
pyyaml==6.0.2
rich==13.9.4
loguru==0.7.3
tqdm==4.67.1

