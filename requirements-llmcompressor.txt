# VLLM AWQ quantization requirements

--extra-index-url https://download.pytorch.org/whl/cu130

# Core AWQ stack (llmcompressor + torch CUDA 13 build)
torch==2.9.0+cu130 ; sys_platform == 'linux'
llmcompressor==0.9.0
