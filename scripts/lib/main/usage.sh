#!/usr/bin/env bash
# Usage documentation for main.sh

main_usage() {
  echo "Usage:"
  echo "  $0 [--vllm|--trt] [4bit|8bit] <chat_model> <tool_model> [deploy_mode]"
  echo "  $0 [--vllm|--trt] [4bit|8bit] chat <chat_model>"
  echo "  $0 [--vllm|--trt] [4bit|8bit] tool <tool_model>"
  echo "  $0 [--vllm|--trt] [4bit|8bit] both <chat_model> <tool_model>"
  echo ""
  echo "Behavior:"
  echo "  • Always runs deployment in background (auto-detached)"
  echo "  • Auto-tails logs (Ctrl+C stops tail, deployment continues)"
  echo "  • Use scripts/stop.sh to stop the deployment"
  echo ""
  echo "Inference Engines:"
  echo "  --trt       → Use TensorRT-LLM engine (default, requires CUDA 13.0+)"
  echo "  --vllm      → Use vLLM engine"
  echo "  --engine=X  → Explicit engine selection (trt or vllm)"
  echo ""
  echo "Quantization:"
  echo "  Omit flag  → auto-detects GPTQ/AWQ/W4A16 hints; otherwise runs 8bit"
  echo "  4bit       → force low-bit chat deployment:"
  echo "               • vLLM: AWQ or GPTQ depending on the model"
  echo "               • TRT:  INT4-AWQ quantization"
  echo "  8bit       → force 8-bit weight quantization:"
  echo "               • vLLM: H100/L40/Ada=native FP8, A100=W8A16 emulated"
  echo "               • TRT:  H100/L40=FP8, A100=INT8-SQ (SmoothQuant)"
  echo ""
  echo "Chat model options:"
  echo "  Float models (8bit auto): SicariusSicariiStuff/Impish_Nemo_12B"
  echo "                           SicariusSicariiStuff/Wingless_Imp_8B"
  echo "                           SicariusSicariiStuff/Impish_Mind_8B"
  echo "                           kyx0r/Neona-12B"
  echo "  GPTQ models (auto):      SicariusSicariiStuff/Impish_Nemo_12B_GPTQ_4-bit-64"
  echo "                           SicariusSicariiStuff/Impish_Nemo_12B_GPTQ_4-bit-128"
  echo "                           SicariusSicariiStuff/Impish_Nemo_12B_GPTQ_4-bit-32"
  echo "  For awq (float weights): SicariusSicariiStuff/Impish_Nemo_12B"
  echo "                           SicariusSicariiStuff/Wingless_Imp_8B"
  echo "                           SicariusSicariiStuff/Impish_Mind_8B"
  echo "                           kyx0r/Neona-12B"
  echo ""
  echo "MoE models:"
  echo "  Qwen/Qwen3-30B-A3B-Instruct-2507"
  echo "  ArliAI/Qwen3-30B-A3B-ArliAI-RpR-v4-Fast"
  echo "  Qwen/Qwen3-Next-80B-A3B-Instruct"
  echo ""
  echo "Tool model options (classifier-only):"
  echo "  yapwithai/yap-longformer-screenshot-intent"
  echo "  (or any compatible transformers classifier repo/path)"
  echo ""
  echo "Required environment variables:"
  echo "  TEXT_API_KEY='secret'             - API authentication key"
  echo "  HF_TOKEN='hf_xxx'                 - Hugging Face access token"
  echo "  MAX_CONCURRENT_CONNECTIONS=<int>  - Capacity guard limit"
  echo ""
  echo "Environment options:"
  echo "  DEPLOY_MODE=both|chat|tool    - Which models to deploy (default: both)"
  echo "  --deploy-mode=both|chat|tool  - CLI override for DEPLOY_MODE"
  echo "  INFERENCE_ENGINE=trt|vllm     - Inference engine (default: trt)"
  echo "  GPU_SM_ARCH=sm80|sm89|sm90    - GPU architecture (auto-detected)"
  echo ""
  echo "Examples:"
  echo "  # Standard TensorRT-LLM deployment (default, 4-bit AWQ)"
  echo "  $0 4bit Qwen/Qwen3-30B-A3B-Instruct-2507 yapwithai/yap-longformer-screenshot-intent"
  echo ""
  echo "  # TensorRT-LLM with 8-bit on L40S (FP8)"
  echo "  $0 8bit SicariusSicariiStuff/Impish_Nemo_12B yapwithai/yap-longformer-screenshot-intent"
  echo ""
  echo "  # TensorRT-LLM with 8-bit on A100 (INT8-SQ)"
  echo "  GPU_SM_ARCH=sm80 $0 8bit SicariusSicariiStuff/Impish_Nemo_12B tool_model"
  echo ""
  echo "  # vLLM deployment (alternative engine)"
  echo "  $0 --vllm SicariusSicariiStuff/Impish_Nemo_12B yapwithai/yap-longformer-screenshot-intent"
  echo ""
  echo "  # 4-bit AWQ for chat (tool classifier stays float)"
  echo "  $0 4bit SicariusSicariiStuff/Impish_Nemo_12B yapwithai/yap-longformer-screenshot-intent"
  echo ""
  echo "  # Chat-only deployment"
  echo "  $0 chat SicariusSicariiStuff/Impish_Nemo_12B"
  echo "  DEPLOY_MODE=chat $0 SicariusSicariiStuff/Impish_Nemo_12B"
  echo ""
  echo "  # Tool-only deployment"
  echo "  $0 tool yapwithai/yap-longformer-screenshot-intent"
  echo "  DEPLOY_MODE=tool $0 yapwithai/yap-longformer-screenshot-intent"
  echo ""
  echo "Quantized model uploads:"
  echo "  --push-quant        Upload freshly built 4-bit exports to Hugging Face"
  echo "  --no-push-quant     Skip uploads (default)"
  exit 1
}

